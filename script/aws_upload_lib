#!/bin/bash
#####################################################################
##                                                                 ##
##  This script is used for uploading data + config to AWS cloud.  ##
##                                                                 ##
#####################################################################
. $BLJ/script/aws_config_lib

S3_DIR="s3://"

# Sync local files to S3 for cloud pipeline execution
aws_sync_inputs_to_s3() {
	[ ${#awsS3} -eq 0 ] && exit_script "Error [ aws_upload_lib.aws_sync_inputs_to_s3() ]: Required property \"awsS3\" is undefined in $blj_aws_config"
	
	# Upload Config
	propFiles=( ${configFiles//,/ } )
	for propFile in ${propFiles[@]}; do upload_s3_file ${EFS_CONFIG}/$(basename "${propFile}") config; done
	
	upload_s3_dir ${EFS_DB} db
	upload_s3_dir ${EFS_INPUT}/$(basename "${inputDirPaths}") input
	upload_s3_file ${EFS_META} metadata
	upload_s3_file ${EFS_PRIMER} primer
}

# Sync local files to S3 for cloud pipeline execution
aws_sync_outputs_to_s3() {
	[ ${#awsS3} -eq 0 ] && exit_script "Error [ aws_upload_lib.aws_sync_outputs_to_s3() ]: Required property \"awsS3\" is undefined in $blj_aws_config"
	upload_s3_dir $BLJ_PROJ pipelines
}

# Sync local files to S3 for cloud pipeline execution
aws_stage_s3_data() {
	[ ${#awsS3} -eq 0 ] && exit_script "Error [ aws_upload_lib.aws_stage_s3_data() ]: Required property \"awsS3\" is undefined in $blj_aws_config"
	
	# Upload Config
	propFiles=( ${configFiles//,/ } )
	for propFile in ${propFiles[@]}; do upload_file_to_aws $propFile ${EFS_CONFIG}; done
	
	# Upload files
	upload_s3_file $metadataFilePath metadata
	upload_s3_file $trimPrimersFilePath primers

	# Upload dirs
	upload_s3_dir $inputDirPaths seq
	upload_s3_dir $humann2NuclDB db
	upload_s3_dir $humann2ProtDB db
	upload_s3_dir $kneaddataDbs db
	upload_s3_dir $krakenDb db
	upload_s3_dir $kraken2Db db
	upload_s3_dir $metaphlan2Db db
	upload_s3_dir $metaphlan2Mpa_pkl db
	upload_s3_dir $qiimePynastAlignDB db
	upload_s3_dir $qiimeRefSeqDB db
	upload_s3_dir $qiimeTaxaDB db
	[ -f "$rdpDb" ] && upload_s3_dir $(dirname rdpDb) db
}

# List all contents of an AWS S3 Bucket
# Param 1 - S3 bucket folder name
aws_s3_ls() {
	aws s3 ls "${S3_DIR}${1}" --recursive --human-readable
}

# Build start script for given Pipeline Config file and reutnr local path
# Param 1 - Pipeline Config file
build_start_script() {
	startScript=~/.aws/run-${awsStack}.sh
	[ -f "${metadataFilePath}" ] && meta="-m=${EFS_META} "
	echo $sheBang > $startScript
	echo "" >> $startScript
	echo "##########################################################" >> $startScript
	echo "##                                                      ##" >> $startScript
	echo "##  Run this script to launch BioLockJ pipeline on AWS  ##" >> $startScript
	echo "##  1) Clear old biolockj Docker images (if any)        ##" >> $startScript
	echo "##  2) Pull the latest biolockj image from Docker Hub   ##" >> $startScript
	echo "##  3) Run dockblj                                      ##" >> $startScript
	echo "##                                                      ##" >> $startScript
	echo "##########################################################" >> $startScript
	echo 'source ${HOME}/.bash_profile' >> $startScript
	echo "clearDock" >> $startScript
	echo "docker pull ${dockerUser}/biolockj_controller:${dockerImgVersion}" >> $startScript
	echo "dockblj -aws ${meta}-i=${EFS_INPUT}/$(basename ${inputDirPaths}) -c=$(get_config ec2)" >> $startScript
	[ "${awsPurgeEfsInputs}" == "Y" ] && echo 'sudo rm -rf $EFS/[cdims]*/*; sudo rm -rf $EFS/primer' >> $startScript
	[ "${awsPurgeEfsOutput}" == "Y" ] && echo 'sudo rm -rf $BLJ_PROJ/*' >> $startScript
	chmod 770 $startScript
	echo $startScript
}

# Execute remote command on the head node
# Param 1 - Remote command
exe_remote_cmd() {
	ssh -o StrictHostKeyChecking=no -i $(key_file) ec2-user@$(get_ec2_public_ip) "$1"
}

# Return the AWS profile tag - if undefined, returns [default]
get_aws_profile() {
	[ ${#awsProfile} -eq 0 ] && awsProfile=default
	echo "[${awsProfile}]"
}

# Return the Nextflow basic config file-path
get_nextflow_config() {
	nfStackDir=~/.aws/nextflow/${awsStack} 
	[ "${USER}" == "ec2-user" ] && nfStackDir=${EFS_CONFIG}/nextflow/${awsStack} 
	[ ! -d "$nfStackDir" ] && mkdir -p $nfStackDir
	echo "$nfStackDir/config"
}

# Get the S3 directory-path
# Param 1 - S3 bucket folder
get_s3_dir() {
	[ ${#1} -eq 0 ] && exit_script "Error [ aws_upload_lib.get_s3_dir() ]:  Missing required parameter - S3 bucket folder name"
	echo "${S3_DIR}${awsS3}/$1"
}

# Build the S3 file/dir-path
# Param 1 - Local Directory of File
# Param 2 - S3 bucket folder
get_s3_name() {
	#[ ! -f "$1" ] && [ ! -d "$1" ] && exit_script "Error [ aws_upload_lib.get_s3_name() ]:  Param 1 = \"${1}\" is not a valid file/dir-path"
	echo "${S3_DIR}${awsS3}/$2/$(basename $1)"
}

# Upload local files to S3
# Copy Config file with updated properties to reference new S3 paths
# Param 1 Pipeline start script
stage_pipeline_on_aws() {
	upload_file_to_aws "$(build_start_script)"
	EC2_NF_CONF="${EFS_CONFIG}/nextflow/$(get_config_name)"
	exe_remote_cmd "[ ! -d $EC2_NF_DIR ] && mkdir -p $EC2_NF_DIR"
	
	#exe_remote_cmd "[ ! -d ${EFS_DB} ] && mkdir ${EFS_DB}"
	#exe_remote_cmd "[ ! -d ${EFS_INPUT} ] && mkdir ${EFS_INPUT}"
	#exe_remote_cmd "[ ! -d ${EFS_META} ] && mkdir ${EFS_META}"
	#exe_remote_cmd "[ ! -d ${EFS_PROJ} ] && mkdir ${EFS_PROJ}"
	#exe_remote_cmd "[ ! -d ${EFS_PRIMER} ] && mkdir ${EFS_PRIMER}"
	#exe_remote_cmd "[ ! -d $EFS_SCRIPT ] && mkdir $EFS_SCRIPT"
	#exe_remote_cmd "[ ! -d $EC2_HOME/.aws ] && mkdir $EC2_HOME/.aws"
	
	stage_aws_confg

	upload_file_to_aws "$blj_aws_config" "${EFS_CONFIG}"
	upload_file_to_aws "$(get_nextflow_config)" "${EC2_NF_DIR}"
	upload_file_to_aws "${BLJ}/resources/aws/ec2_head_node_profile"
	upload_pipeline_inputs_to_aws
	upload_dir_to_aws "${BLJ}/script" "${EFS}"
	[ -d "${BLJ_SUP}/script" ] && upload_dir_to_aws "${BLJ_SUP}/script" "${EFS}"
	[ -f "$(get_docker_job_flag)" ] && upload_file_to_aws "$(get_docker_job_flag)"
	exe_remote_cmd "mv ~/.bash_profile ~/.bash_profile~; mv ~/ec2_head_node_profile ~/.bash_profile"
	
	#exe_remote_cmd "sudo chmod -R 777 ${EFS}"
	#exe_remote_cmd "sudo chmod -R g+s ${EFS}"
	exe_remote_cmd "x=$(find ${EFS} -name .DS_Store); [ ${#x} -gt 0 ] && rm $x"
}

# Stage AWS config and credentials files to push to AWS cloud
stage_aws_confg() {
	[ ! -f "${HOME}/.aws/config" ] && exit_script "Error [ aws_upload_lib.stage_aws_confg() ]: Required file not found: ${HOME}/.aws/config"
	[ ! -f "${HOME}/.aws/credentials" ] && exit_script "Error [ aws_upload_lib.stage_aws_confg() ]: Required file not found: ${HOME}/.aws/credentials"
	[ ${#aws_access_key_id} -eq 0 ] || [ ${#aws_secret_access_key} -eq 0 ] && exit_script "Error [ aws_upload_lib.stage_aws_confg() ]: Login credentials not found: ${HOME}/.aws/credentials"
	stageDir="${HOME}/.aws/stage"
	[ ! -d "${HOME}/.aws/stage" ] && mkdir "$stageDir"
	echo "$(get_aws_profile)" > "$stageDir/config"
	echo "region = ${awsRegion}" >> "$stageDir/config"
	echo "output = text" >> "$stageDir/config"
	echo "Created file:  $stageDir/config"
	cat "$stageDir/config" 
	echo "$(get_aws_profile)" > "$stageDir/credentials"
	echo "aws_access_key_id = ${aws_access_key_id}" >> "$stageDir/credentials"
	echo "aws_secret_access_key = ${aws_secret_access_key}" >> "$stageDir/credentials"
	echo "Created file:  $stageDir/credentials"
	cat "$stageDir/credentials"
	upload_file_to_aws "$stageDir/config" "${EC2_HOME}/.aws"
	upload_file_to_aws "$stageDir/credentials" "$EC2_HOME}/.aws"
	upload_file_to_aws "$blj_aws_config" "${EC2_HOME}/.aws"
}

# The method creates a new version of the pipeline Config files by replacing 
upload_config_to_aws() {
	primaryConfig=$(get_config local)
	myAwsS3=$(get_prop "$primaryConfig" "aws.s3" "${awsS3}")
	[ ${#myAwsS3} -eq 0 ] && set_prop "$primaryConfig" "aws.s3" "${awsS3}"
	configFiles=$(get_blj_prop configFiles)
	aws_log "Uploaded Config files: $configFiles"
	propFiles=( ${configFiles//,/ } )
	for propFile in ${propFiles[@]}; do upload_file_to_aws $propFile ${EFS_CONFIG}; done
}

# Upload local directory up to the ec2 head node, if no target dir specified use EC2 ${HOME}: /Users/mike/git/blj_support/resources/test/data/r16s_fastq
# Param 1 - Local dir
# Param 2 - (optional) AWS ec2 directory-path 
upload_dir_to_aws() {
	if [ $# -eq 2 ]; then target="${2}"; else target=$EC2_HOME; fi
	aws_log "Uploading directory \"${1}\" to AWS --> \"$target\""
	scp -pro StrictHostKeyChecking=no -i $(key_file) "${1}" ec2-user@$(get_ec2_public_ip):${target}
}

# Upload local file-path up to the ec2 head node, if no target dir specified use EC2 ${HOME}
# Param 1 - Local file
# Param 2 - (optional) AWS ec2 file-path 
upload_file_to_aws() {
	if [ $# -eq 2 ]; then target="$2"; else target=$EC2_HOME; fi
	aws_log "Uploading file \"$1\" to AWS --> \"$target\""
	scp -pro StrictHostKeyChecking=no -i $(key_file) "$1" ec2-user@$(get_ec2_public_ip):${target}
}

# Stage data for pipeline onto EFS volume
# Param 1 - AWS run-pipeline script
upload_pipeline_inputs_to_aws() {
	aws_log "Staging pipeline inputs..."
	if [ -d "${inputDirPaths}" ]; then
		upload_dir_to_aws ${inputDirPaths} ${EFS_INPUT}
	else
		exit_script "Error [ aws_upload_lib.upload_pipeline_inputs_to_aws() ]: Required Config inputDirPaths is not a valid directory"
	fi
	if [ -d "$metaphlan2Db" ] || [ -f "$metaphlan2Mpa_pkl" ]; then
		if [ -d "$metaphlan2Db" ] && [ -f "$metaphlan2Mpa_pkl" ]; then
			upload_dir_to_aws ${metaphlan2Db} ${EFS_DB}
			upload_file_to_aws ${metaphlan2Mpa_pkl} ${EFS_DB}
		else
			exit_script "Error [ aws_upload_lib.upload_pipeline_inputs_to_aws() ]: Required Config missing - if any defined, all must be defined {\"metaphlan2Db\", \"metaphlan2Mpa_pkl\" }"
		fi
	fi
	if [ -f "$qiimePynastAlignDB" ] || [ -f "$qiimeRefSeqDB" ] || [ -f "$qiimeTaxaDB" ]; then
		if [ -f "$qiimePynastAlignDB" ] && [ -f "$qiimeRefSeqDB" ] && [ -f "$qiimeTaxaDB" ]; then
			upload_file_to_aws ${qiimePynastAlignDB} ${EFS_DB}
			upload_file_to_aws ${qiimeRefSeqDB} ${EFS_DB}
			upload_file_to_aws ${qiimeTaxaDB} ${EFS_DB}	
		else
			exit_script "Error [ aws_upload_lib.upload_pipeline_inputs_to_aws() ]: Required Config missing - if any defined, all must be defined { \"qiimePynastAlignDB\", \"qiimeRefSeqDB\", \"qiimeTaxaDB\" }"
		fi
	fi
	[ -f "$metadataFilePath" ] && upload_file_to_aws ${metadataFilePath} ${EFS_META}
	[ -f "$trimPrimersFilePath" ] && upload_file_to_aws ${trimPrimersFilePath} ${EFS_PRIMER}
	[ -d "$kneaddataDbs" ] && upload_dir_to_aws ${kneaddataDbs} ${EFS_DB}
	[ -d "$krakenDb" ] && upload_dir_to_aws ${krakenDb} ${EFS_DB}
	[ -d "$kraken2Db" ] && upload_dir_to_aws ${kraken2Db} ${EFS_DB}
	[ -d "$humann2NuclDB" ] && upload_dir_to_aws ${humann2NuclDB} ${EFS_DB}	
	[ -d "$humann2ProtDB" ] && upload_dir_to_aws ${humann2ProtDB} ${EFS_DB}	
	[ -f "$rdpDb" ] && upload_dir_to_aws $(dirname ${rdpDb}) ${EFS_DB}	
	upload_config_to_aws
}

# Upload local directory to AWS S3 Bucket folder (if modified)
# Param 1 - Local directory
# Param 2 - S3 bucket folder
upload_s3_dir() {
	[ $# -ne 2 ] || [ "${1:0:5}" == "$S3_DIR" ] && return
	[ ! -d "$1" ] && exit_script "Error [ aws_upload_lib.upload_s3_dir() ]: Local path is not a valid directory -->  $1"
	aws s3 sync "$1" "$(get_s3_dir $2)/$(basename $1)" --exclude *.DS_Store
}
 
# Upload local file to AWS S3 Bucket folder (if modified)
# Param 1 - Local file
# Param 2 - S3 bucket folder
upload_s3_file() {
	[ $# -ne 2 ] || [ "${2:0:5}" == "$S3_DIR" ] && return
	[ ! -f "$1" ] && exit_script "Error [ aws_upload_lib.upload_s3_file() ]: : Local path is not a valid file -->  $1"
	aws s3 cp "$1" "$(get_s3_dir $2)/$(basename $1)"
}
