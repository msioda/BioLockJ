#!/bin/bash
#####################################################################
##                                                                 ##
##  This script is used for uploading data + config to AWS cloud.  ##
##                                                                 ##
##  s3://$awsS3           - Top level $BLJ bucket                  ##
##  s3://$awsS3/config    - $BLJ_CONFIG folder with DB sub-folders ##
##  s3://$awsS3/db        - $BLJ_DB folder with DB sub-folders     ##
##  s3://$awsS3/metadata  - $BLJ_META folder for metadata files    ##
##  s3://$awsS3/pipelines - $BLJ_PROJ folder for pipeline output   ##
##  s3://$awsS3/primers   - $BLJ_PRIMER folder for seq primers     ##
##  s3://$awsS3/seq       - $BLJ_SEQ folder for sequence files     ##
##                                                                 ##
#####################################################################
. $BLJ/script/aws_config_lib

S3_DIR="s3://"

# Sync local files to S3 for cloud pipeline execution
aws_sync_s3() {
	[ ${#awsS3} -eq 0 ] && exit_script "Error [ aws_upload_lib.aws_sync_s3() ]: Required property \"awsS3\" is undefined in $blj_aws_config"
	
	# Upload files
	upload_s3_file $metadataFilePath metadata
	upload_s3_file $trimPrimersFilePath primers

	# Upload dirs
	upload_s3_dir $inputDirPaths seq
	upload_s3_dir $humann2NuclDB db
	upload_s3_dir $humann2ProtDB db
	upload_s3_dir $kneaddataDbs db
	upload_s3_dir $krakenDb db
	upload_s3_dir $kraken2Db db
	upload_s3_dir $metaphlan2Db db
	upload_s3_dir $metaphlan2Mpa_pkl db
	upload_s3_dir $qiimePynastAlignDB db
	upload_s3_dir $qiimeRefSeqDB db
	upload_s3_dir $qiimeTaxaDB db
	[ -f "$rdpDb" ] && upload_s3_dir $(dirname rdpDb) db
}

# List all contents of an AWS S3 Bucket
# TEST: aws_s3_ls $awsS3/r16s_fastq
# Param 1 - S3 bucket folder name
aws_s3_ls() {
	aws s3 ls "${S3_DIR}${1}" --recursive --human-readable
}

# Build the S3 file/dir-path
# Param 1 - Local Directory of File
# Param 2 - S3 bucket folder
get_s3_name() {
	#[ ! -f "$1" ] && [ ! -d "$1" ] && exit_script "Error [ aws_upload_lib.get_s3_name() ]:  Param 1 = \"${1}\" is not a valid file/dir-path"
	echo "${S3_DIR}${awsS3}/$2/$(basename $1)"
}

# Get the S3 directory-path
# Param 1 - S3 bucket folder
get_s3_dir() {
	[ ${#1} -eq 0 ] && exit_script "Error [ aws_upload_lib.get_s3_dir() ]:  Missing required parameter - S3 bucket folder name"
	echo "${S3_DIR}${awsS3}/$1"
}

# Upload local file or directory to the ec2 head node
# Param 1 - Local file
# Param 2 - AWS ec2 Head Node directory 
stage_file_to_head_node() {
	target=/home/ec2-user/
	[ $# -eq 2 ] && target="$2"
	scp -o StrictHostKeyChecking=no -i $(key_file) $1 ec2-user@$(get_blj_prop publicHost):$target
}

# Upload local file or directory to the ec2 head node
# Param 1 - Local dir
# Param 2 - AWS ec2 Head Node directory 
stage_dir_to_head_node() {
	target=/home/ec2-user/
	[ $# -eq 2 ] && target="$2"
	for f in "$1/*"; do
		stage_file_to_head_node "$f" ${2}
	done
}

# Upload local directory to AWS S3 Bucket folder (if modified)
# Param 1 - Local directory
# Param 2 - S3 bucket folder
upload_s3_dir() {
	[ $# -ne 2 ] || [ "${1:0:5}" == "$S3_DIR" ] && return
	[ ! -d "$1" ] && exit_script "Error [ aws_upload_lib.upload_s3_dir() ]: Local path is not a valid directory -->  $1"
	aws s3 sync "$1" "$(get_s3_dir $2)/$(basename $1)" --exclude *.DS_Store
}
 
# Upload local file to AWS S3 Bucket folder (if modified)
# Param 1 - Local file
# Param 2 - S3 bucket folder
upload_s3_file() {
	[ $# -ne 2 ] || [ "${2:0:5}" == "$S3_DIR" ] && return
	[ ! -f "$1" ] && exit_script "Error [ aws_upload_lib.upload_s3_file() ]: : Local path is not a valid file -->  $1"
	aws s3 cp "$1" "$(get_s3_dir $2)/$(basename $1)"
}
