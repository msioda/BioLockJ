#!/bin/bash
#####################################################################
##                                                                 ##
##  This script is used for uploading data + config to AWS cloud.  ##
##                                                                 ##
##  s3://$awsS3           - Top level $BLJ bucket                  ##
##  s3://$awsS3/config    - $BLJ_CONFIG folder with DB sub-folders ##
##  s3://$awsS3/db        - $BLJ_DB folder with DB sub-folders     ##
##  s3://$awsS3/metadata  - $BLJ_META folder for metadata files    ##
##  s3://$awsS3/pipelines - $BLJ_PROJ folder for pipeline output   ##
##  s3://$awsS3/primers   - $BLJ_PRIMER folder for seq primers     ##
##  s3://$awsS3/seq       - $BLJ_SEQ folder for sequence files     ##
##                                                                 ##
#####################################################################
. $BLJ/script/aws_upload_lib

AWS_CONFIG=~/.aws_config
AWS_DOCKER_JOB_FLAG=~/.aws/.${awsStack}-CREATED_DOCKER_JOB_DEFS
HEAD=/home/ec2-user
EFS=/mnt/efs
headConfig=$HEAD/config
headScript=$HEAD/script
headDB=$EFS/db
headInput=$EFS/input
headMeta=$EFS/metadata
headPrimer=$EFS/primer
headPipelines=$EFS/pipelines

# Build Docker Job defs to use with label references in Nextflow main.nf
build_docker_job_defs() {
	dockerModules=$(docker search ${dockerUser} | grep -E ^${dockerUser} ) 
	jobVcpus=2
	jobRam=1024
	IFS=$'\t'
	[ ${#dockerModules} -eq 0 ] && aws_log "No Docker images found for Docker Account \"${dockerUser}\"" && return
	aws_log "Building job-definitions"
	echo ${dockerModules} | while read -r line; do
		[ $(echo $line | grep -c blj_basic) -gt 0 ] && continue
		module=$(echo $line | cut -f 1 -d " ")
		jobImg="${module}:${dockerImgVersion}"
		jobLabel=${jobImg/\//_} 
		jobLabel=${jobLabel/:/_} 
	    jobConfig="{ 
	    		\"image\": \"${jobImg}\",
			\"vcpus\": ${jobVcpus},
			\"memory\": ${jobRam},
			\"command\": [ \"/bin/bash\" ],
			\"jobRoleArn\": \"$(get_stack_param ECSTaskRole)\",
			\"volumes\": [ { \"host\": { \"sourcePath\": \"/mnt/efs\" }, \"name\": \"efs\" } ],
			\"mountPoints\": [ { \"containerPath\": \"/efs\", \"readOnly\": false, \"sourceVolume\": \"efs\" } ],
			\"readonlyRootFilesystem\": false,
			\"privileged\": true
		}"
		jobDef="${awsStack}_${jobLabel}"
		jobName=$(aws batch describe-job-definitions --status ACTIVE --job-definition-name ${jobDef} --query "jobDefinitions[*].jobDefinitionName")
		if [ ${#jobName} -eq 0 ]; then
			registeredJob=$(aws batch register-job-definition --job-definition-name $jobDef --type container --container-properties "${jobConfig}")
			jobName=$(echo $registeredJob | grep job-definition | sed 's/^.*job-definition/job-definition:\//' | awk '//{print $1}' )
			aws_log "Registered new Docker job-definition: ${jobName}"
		else
			aws_log "Found existing Docker job-definition: ${jobName}"
		fi
		prop=$(get_blj_prop "image_${jobLabel}" ${jobName})
		aws_log "Saved Docker job-definition label: image_${jobLabel}=${prop}"
	done
	touch $AWS_DOCKER_JOB_FLAG
}


# Build start script for given Config file
# Param 1 - Pipeline Config file
build_start_script() {
	[ -f "${metadataFilePath}" ] && meta="-m=$headMeta/$(basename ${metadataFilePath}) "
	echo $sheBang > $(get_start_script)
	echo "dockblj ${meta}-i=$headInput -aws -c=$1" >> $(get_start_script)
	chmod 770 $(get_start_script)
	echo $(get_start_script)
}

# Connect to running head node
connect_head() {
	aws_log "Opening SSH tunnel to Head Node...$(get_blj_prop publicHost)"
	ssh -o StrictHostKeyChecking=no -i $(key_file) ec2-user@$(get_blj_prop publicHost)
}

# Execute remote command on the head node
# Param 1 - Remote command
exe_remote() {
	ssh -o StrictHostKeyChecking=no -i $(key_file) ec2-user@$(get_blj_prop publicHost) "$1"
}

get_config() {
	configFiles=$(get_blj_prop configFiles)
	propFiles=( ${configFiles//,/ } )
	numConfig=${#propFiles[@]}
	((numConfig--))
	echo "$headConfig/$(basename ${propFiles[$numConfig]})"
}

# Get a stack parameter for the configured stack 
# Param 1 - Param name
get_stack_param() {
	myParam=$(aws cloudformation describe-stacks --stack-name $(get_blj_prop awsStack) --query "Stacks[*].Outputs[?OutputKey=='$1'].OutputValue")
	[ ${#myParam} -eq 0 ] && return
	echo "${myParam}"
}

get_start_script() {
	echo ~/.aws/run-${awsStack}.sh
}

# Upload local files to S3
# Copy Config file with updated properties to reference new S3 paths
init_head_node() {

	exe_remote "[ ! -d $headConfig ] && mkdir $headConfig"
	exe_remote "[ ! -d $headScript ] && mkdir $headScript"
	exe_remote "[ ! -d $headDB ] && mkdir $headDB"
	exe_remote "[ ! -d $headInput ] && mkdir $headInput"
	exe_remote "[ ! -d $headMeta ] && mkdir $headMeta"
	exe_remote "[ ! -d $headPipelines ] && mkdir $headPipelines"
	exe_remote "[ ! -d $headPrimer ] && mkdir $headPrimer"
	
	stage_pipeline_data
	update_head_config

	[ -f "$AWS_DOCKER_JOB_FLAG" ] && stage_file_to_head_node $AWS_DOCKER_JOB_FLAG
	stage_file_to_head_node $BLJ/resources/aws/ec2_head_node_profile 
	stage_dir_to_head_node $AWS_CONFIG $headConfig
	stage_dir_to_head_node $BLJ/script $headScript
	exe_remote "mv ~/.bash_profile ~/.bash_profile~; mv ~/ec2_head_node_profile ~/.bash_profile"

	stage_file_to_head_node $(build_start_script $(get_config))
	exe_remote "chmod 770 $HEAD/$(basename $(get_start_script))"
	
	[ -x "$(which docker)" ] && exe_remote $HEAD/$(basename $(get_start_script))
}

# Launch a new ec2 head node
launch_ec2_head_node() {
	refresh_aws_cache
	[ -x "$(which docker)" ] && build_docker_job_defs
	runTime=0
	ami="${awsAmi}"
	stack="${awsStack}"
	secureGroup="$(get_stack_param BastionSecurityGroup)"
	subnet="$(get_stack_param Subnet1)"
	templateId="$(get_stack_param HeadNodeLaunchTemplateId)"
	if [ {#ami} -eq 0 ] || [ {#stack} -eq 0 ] || [ {#secureGroup} -eq 0 ] || [ {#subnet} -eq 0 ] || [ {#templateId} -eq 0 ]; then
		exit_script "Error [ aws_head_node_lib.launch_ec2_head_node() ]: Cannot launch EC2 - required parameters not found in $blj_aws_config"
	fi
	instanceID=$(aws ec2 run-instances --count 1 --key-name $stack --image-id $ami --security-group-ids $secureGroup  \
		--tag-specifications "ResourceType=instance,Tags={Key=Name,Value='HeadNode'}" --subnet-id $subnet \
		--launch-template LaunchTemplateId=$templateId --instance-type $awsEc2InstanceType --query "Instances[].InstanceId" )
	printf "Launching EC2 Instance, please wait."
	systemStatus=init
	while [ "$systemStatus" != "ok" ]; do
		systemStatus=$(aws ec2 describe-instance-status --instance-ids ${instanceID} --query "InstanceStatuses[*].SystemStatus.Status")
		printf "."
		sleep 10s
		runTime=$((runTime+10))  
	done
	aws_log "InstanceID [ ${instanceID} ] created in $runTime seconds!"
	set_blj_prop instanceID ${instanceID}
	set_blj_prop publicHost $(aws ec2 describe-instances --instance-ids ${instanceID} --query "Reservations[].Instances[].PublicDnsName")
	refresh_aws_cache
	keyFound=$(cat ~/.ssh/known_hosts| grep -c ${publicHost})
	[ ${keyFound} -gt 0 ] && ssh-keygen -f ~/.ssh/known_hosts -R ${publicHost}
	init_head_node
	connect_head
}

# Check if prop is the target prop prop, or its mapped prop name. 
# Param 1 - prop
# Param 2 - target
prop_exists() {
	[ $# -eq 2 ] && [ "$1" ==  "$2" ] || [ "$1" == $(map_property_name "$2") ]
}

# Stage data for pipeline onto EFS volume
stage_pipeline_data() {
	aws_log "Staging pipeline data..."

	if [ -d "$inputDirPaths" ]; then
		stage_dir_to_head_node ${inputDirPaths} $headInput
	else
		exit_script "Error [ aws_head_node_lib.stage_pipeline_data() ]: Required Config inputDirPaths undefined"
	fi
	
	[ -f "$metadataFilePath" ] && stage_file_to_head_node ${metadataFilePath} $headMeta
	[ -f "$trimPrimersFilePath" ] && stage_file_to_head_node ${trimPrimersFilePath} $headPrimer
	[ -d "$kneaddataDbs" ] && stage_dir_to_head_node ${kneaddataDbs} $headDB
	[ -d "$krakenDb" ] && stage_dir_to_head_node ${krakenDb} $headDB
	[ -d "$kraken2Db" ] && stage_dir_to_head_node ${kraken2Db} $headDB
	
	if [ -d "$metaphlan2Db" ] || [ -f "$metaphlan2Mpa_pkl" ]; then
		if [ -d "$metaphlan2Db" ] && [ -f "$metaphlan2Mpa_pkl" ]; then
			stage_dir_to_head_node ${metaphlan2Db} $headDB
			stage_file_to_head_node ${metaphlan2Mpa_pkl} $headDB
		else
			exit_script "Error [ aws_head_node_lib.stage_pipeline_data() ]: Required Config missing - if any defined all must be defined \"metaphlan2Db\" and \"metaphlan2Mpa_pkl\""
		fi
	fi
	
	if [ -f "$qiimePynastAlignDB" ] || [ -f "$qiimeRefSeqDB" ] || [ -f "$qiimeTaxaDB" ]; then
		if [ -f "$qiimePynastAlignDB" ] && [ -f "$qiimeRefSeqDB" ] && [ -f "$qiimeTaxaDB" ]; then
			stage_file_to_head_node ${qiimePynastAlignDB} $headDB
			stage_file_to_head_node ${qiimeRefSeqDB} $headDB
			stage_file_to_head_node ${qiimeTaxaDB} $headDB	
		else
			exit_script "Error [ aws_head_node_lib.stage_pipeline_data() ]: Required Config missing - if any defined all must be defined \"qiimePynastAlignDB\", \"qiimeRefSeqDB\", \"qiimeTaxaDB\""
		fi
	fi
			
	[ -d "$humann2NuclDB" ] && stage_dir_to_head_node ${humann2NuclDB} $headDB	
	[ -d "$humann2ProtDB" ] && stage_dir_to_head_node ${humann2ProtDB} $headDB	
	[ -f "$rdpDb" ] && stage_dir_to_head_node $(dirname ${rdpDb}) $headDB	
}

# The method creates a new version of the pipeline Config files by replacing 
update_head_config() {
	[ ! -d "$AWS_CONFIG" ] && mkdir $AWS_CONFIG && aws_log "Creating the staging directory:  $AWS_CONFIG"
	rm $AWS_CONFIG/*
	configFiles=$(get_blj_prop configFiles)
	aws_log "Generating new Config files with properties that to point to EFS"
	newPropFiles=
	propFiles=( ${configFiles//,/ } )
	for propFile in ${propFiles[@]}; do
		headPath=$headConfig/$(basename $propFile)
		update_head_config_file $propFile
		[ ${#newPropFiles} -gt 0 ] && newPropFiles=",${newPropFiles}"
		newPropFiles=${headPath}${newPropFiles}
	done
	set_blj_prop configFiles $newPropFiles
	update_head_config_file $blj_aws_config
	refresh_aws_cache
}


# Change the file/dir-path to the new S3 path
# Param 1 - Config file
update_head_config_file() {
	outFile="$AWS_CONFIG/$(basename $1)"
	touch $outFile
	while read line; do
		line=$(echo ${line} | xargs)
		prop=
		val=
		if [ "${line/=/}" != "${line}" ] && [ "${line:0:1}" != "#" ]; then
			prop=$(echo ${line} | cut -d '=' -f1)
			val=$(eval "echo ${line} | cut -d '=' -f2")
			if $(prop_exists "$prop" "pipeline.defaultProps"); then
				echo "$prop=$headConfig/$(basename $val)" >> $outFile	
			elif $(prop_exists "$prop" "input.dirPaths"); then
				echo "$prop=$headInput/$(basename $val)" >> $outFile	
			elif $(prop_exists "$prop" "metadata.filePath"); then
				echo "$prop=$headMeta/$(basename $val)" >> $outFile	
			elif $(prop_exists "$prop" "trimPrimers.filePath"); then
				echo "$prop=$headPrimer/$(basename $val)" >> $outFile
			elif $(prop_exists "$prop" "kneaddata.dbs") && [ "$val" != "$DB" ]; then
				echo "$prop=$headDB/$(basename $val)" >> $outFile
			elif $(prop_exists "$prop" "kraken.db") && [ "$val" != "$DB" ]; then
				echo "$prop=$headDB/$(basename $val)" >> $outFile
			elif $(prop_exists "$prop" "kraken2.db") && [ "$val" != "$DB" ]; then
				echo "$prop=$headDB/$(basename $val)" >> $outFile
			elif $(prop_exists "$prop" "metaphlan2.db") && [ "$val" != "$DB" ]; then
				echo "$prop=$headDB/$(basename $val)" >> $outFile
			elif $(prop_exists "$prop" "metaphlan2.mpa_pkl") && [ "$val" != "$DB" ]; then
				echo "$prop=$headDB/$(basename $val)" >> $outFile
			elif $(prop_exists "$prop" "qiime.pynastAlignDB") && [ "$val" != "$DB" ]; then
				echo "$prop=$headDB/$(basename $val)" >> $outFile
			elif $(prop_exists "$prop" "qiime.refSeqDB") && [ "$val" != "$DB" ]; then
				echo "$prop=$headDB/$(basename $val)" >> $outFile
			elif $(prop_exists "$prop" "qiime.taxaDB") && [ "$val" != "$DB" ]; then
				echo "$prop=$headDB/$(basename $val)" >> $outFile
			elif $(prop_exists "$prop" "humann2.nuclDB") && [ "$val" != "$HN2_NUCL_DB" ]; then
				echo "$prop=$headDB/$(basename $val)" >> $outFile
			elif $(prop_exists "$prop" "humann2.protDB") && [ "$val" != "$HN2_PROT_DB" ]; then
				echo "$prop=$headDB/$(basename $val)" >> $outFile
			elif $(prop_exists "$prop" "rdp.db") && [ "$val" != "$RDP_DB" ]; then
				dirPath=$(dirname $val)
				parDir=$(basename $dirPath)
				echo "$prop=$headDB/$parDir/$(basename $val)" >> $outFile
			else
				echo "${line}" >> $outFile
			fi
		else
			echo "${line}" >> $outFile
		fi
	done < $1
	echo ${outFile}
}
