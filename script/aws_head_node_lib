#!/bin/bash
#####################################################################
##                                                                 ##
##  This script is used for uploading data + config to AWS cloud.  ##
##                                                                 ##
##  s3://$awsS3           - Top level $BLJ bucket                  ##
##  s3://$awsS3/config    - $BLJ_CONFIG folder with DB sub-folders ##
##  s3://$awsS3/db        - $BLJ_DB folder with DB sub-folders     ##
##  s3://$awsS3/metadata  - $BLJ_META folder for metadata files    ##
##  s3://$awsS3/pipelines - $BLJ_PROJ folder for pipeline output   ##
##  s3://$awsS3/primers   - $BLJ_PRIMER folder for seq primers     ##
##  s3://$awsS3/seq       - $BLJ_SEQ folder for sequence files     ##
##                                                                 ##
#####################################################################
. $BLJ/script/aws_config_lib

AWS_CONFIG=~/.aws_config
S3_DIR="s3://"

# Sync local files to S3 for cloud pipeline execution
aws_sync_s3() {
	[ ${#awsS3} -eq 0 ] && exit_script "Error [ aws_head_node_lib.aws_sync_s3() ]: Required property \"awsS3\" is undefined in $blj_aws_config"
	
	# Upload files
	upload_s3_dir $metadataFilePath metadata
	upload_s3_dir $trimPrimersFilePath primers
	upload_s3_file $rdpDb db
	
	# Upload dirs
	upload_s3_dir $inputDirPaths seq
	upload_s3_dir $humann2NuclDB db
	upload_s3_dir $humann2ProtDB db
	upload_s3_dir $kneaddataDbs db
	upload_s3_dir $krakenDb db
	upload_s3_dir $kraken2Db db
	upload_s3_dir $metaphlan2Db db
	upload_s3_dir $metaphlan2Mpa_pkl db
	upload_s3_dir $qiimePynastAlignDB db
	upload_s3_dir $qiimeRefSeqDB db
	upload_s3_dir $qiimeTaxaDB db
}

# List all contents of an AWS S3 Bucket
# TEST: aws_s3_ls $awsS3/r16s_fastq
# Param 1 - S3 bucket folder name
aws_s3_ls() {
	aws s3 ls "${S3_DIR}${1}" --recursive --human-readable
}

# Build Docker Job defs to use with label references in Nextflow main.nf
build_docker_job_defs() {
	dockerModules=$(docker search ${dockerUser} | grep -E ^${dockerUser} ) 
	jobVcpus=2
	jobRam=1024
	IFS=$'\t'
	[ ${#dockerModules} -eq 0 ] && aws_log "No Docker images found for Docker Account \"${dockerUser}\"" && return
	aws_log "Building job-definitions"
	echo ${dockerModules} | while read -r line; do
		[ $(echo $line | grep -c blj_basic) -gt 0 ] && continue
		module=$(echo $line | cut -f 1 -d " ")
		jobImg="${module}:${dockerImgVersion}"
		jobLabel=${jobImg/\//_} 
		jobLabel=${jobLabel/:/_} 
	    jobConfig="{ 
	    		\"image\": \"${jobImg}\",
			\"vcpus\": ${jobVcpus},
			\"memory\": ${jobRam},
			\"command\": [ \"/bin/bash\" ],
			\"jobRoleArn\": \"$(get_stack_param ECSTaskRole)\",
			\"volumes\": [ { \"host\": { \"sourcePath\": \"/mnt/efs\" }, \"name\": \"efs\" } ],
			\"mountPoints\": [ { \"containerPath\": \"/efs\", \"readOnly\": false, \"sourceVolume\": \"efs\" } ],
			\"readonlyRootFilesystem\": false,
			\"privileged\": true
		}"
		jobDef="${awsStack}_${jobLabel}"
		jobName=$(aws batch describe-job-definitions --status ACTIVE --job-definition-name ${jobDef} --query "jobDefinitions[*].jobDefinitionName")
		if [ ${#jobName} -eq 0 ]; then
			registeredJob=$(aws batch register-job-definition --job-definition-name $jobDef --type container --container-properties "${jobConfig}")
			jobName=$(echo $registeredJob | grep job-definition | sed 's/^.*job-definition/job-definition:\//' | awk '//{print $1}' )
			aws_log "Registered new Docker job-definition: ${jobName}"
		else
			aws_log "Found existing Docker job-definition: ${jobName}"
		fi
		prop=$(get_blj_prop "image_${jobLabel}" ${jobName})
		aws_log "Saved Docker job-definition label: image_${jobLabel}=${prop}"
	done
	touch ~/.aws/.${awsStack}-CREATED_DOCKER_JOB_DEFS
}

# Connect to running head node
connect_head() {
	aws_log "Opening SSH tunnel to Head Node...$(get_blj_prop publicHost)"
	ssh -o StrictHostKeyChecking=no -i $(key_file) ec2-user@$(get_blj_prop publicHost)
	
	#mv ~/.bash_profile ~/.bash_profile~
	#mv ~/ec2_head_node_profile ~/.bash_profile
	#. ~/.bash_profile
	#build_docker_job_defs
}

# Execute remote command on the head node
# Param 1 - Remote command
exe_remote() {
	ssh -o StrictHostKeyChecking=no -i $(key_file) ec2-user@$(get_blj_prop publicHost) "$1"
}

# Get a stack parameter for the configured stack 
# Param 1 - Param name
get_stack_param() {
	myParam=$(aws cloudformation describe-stacks --stack-name $(get_blj_prop awsStack) --query "Stacks[*].Outputs[?OutputKey=='$1'].OutputValue")
	[ ${#myParam} -eq 0 ] && return
	echo "${myParam}"
}

# Build the S3 file/dir-path
# Param 1 - Local Directory of File
# Param 2 - S3 bucket folder
get_s3_name() {
	#[ ! -f "$1" ] && [ ! -d "$1" ] && exit_script "Error [ aws_head_node_lib.get_s3_name() ]:  Param 1 = \"${1}\" is not a valid file/dir-path"
	echo "${S3_DIR}${awsS3}/$2/$(basename $1)"
}

# Get the S3 directory-path
# Param 1 - S3 bucket folder
get_s3_dir() {
	[ ${#1} -eq 0 ] && exit_script "Error [ aws_head_node_lib.get_s3_dir() ]:  Missing required parameter - S3 bucket folder name"
	echo "${S3_DIR}${awsS3}/$1"
}

# Upload local files to S3
# Copy Config file with updated properties to reference new S3 paths
init_head_node() {
	update_s3_config_files
	aws_sync_s3
	
	stage_to_head_node $BLJ/resources/aws/ec2_head_node_profile 
	stage_to_head_node $blj_aws_config
	stage_to_head_node $BLJ/script
	
	exe_remote "mv ~/.bash_profile ~/.bash_profile~; mv ~/ec2_head_node_profile ~/.bash_profile"
	
	#exe_remote "[ ! -d /home/ec2-user/config ] && mkdir /home/ec2-user/config"
	#configFiles=$(get_blj_prop configFiles)
	#propFiles=( ${configFiles//,/ } ) 
	#for propFile in ${propFiles[@]}; do
	#	stage_to_head_node $propFile /home/ec2-user/config
	#done
}

# Launch a new ec2 head node
launch_ec2_head_node() {
	refresh_aws_cache
	[ -x "$(which docker)" ] && build_docker_job_defs
	runTime=0
	ami="${awsAmi}"
	stack="${awsStack}"
	secureGroup="$(get_stack_param BastionSecurityGroup)"
	subnet="$(get_stack_param Subnet1)"
	templateId="$(get_stack_param HeadNodeLaunchTemplateId)"
	if [ {#ami} -eq 0 ] || [ {#stack} -eq 0 ] || [ {#secureGroup} -eq 0 ] || [ {#subnet} -eq 0 ] || [ {#templateId} -eq 0 ]; then
		exit_script "Error [ aws_head_node_lib.launch_ec2_head_node() ]: Cannot launch EC2 - required parameters not found in $blj_aws_config"
	fi
	instanceID=$(aws ec2 run-instances --count 1 --key-name $stack --image-id $ami --security-group-ids $secureGroup  \
		--tag-specifications "ResourceType=instance,Tags={Key=Name,Value='HeadNode'}" --subnet-id $subnet \
		--launch-template LaunchTemplateId=$templateId --instance-type $awsEc2InstanceType --query "Instances[].InstanceId" )
	printf "Launching EC2 Instance, please wait."
	systemStatus=init
	while [ "$systemStatus" != "ok" ]; do
		systemStatus=$(aws ec2 describe-instance-status --instance-ids ${instanceID} --query "InstanceStatuses[*].SystemStatus.Status")
		printf "."
		sleep 10s
		runTime=$((runTime+10))  
	done
	aws_log "InstanceID [ ${instanceID} ] created in $runTime seconds!"
	set_blj_prop instanceID ${instanceID}
	set_blj_prop publicHost $(aws ec2 describe-instances --instance-ids ${instanceID} --query "Reservations[].Instances[].PublicDnsName")
	refresh_aws_cache
	keyFound=$(cat ~/.ssh/known_hosts| grep -c ${publicHost})
	[ ${keyFound} -gt 0 ] && ssh-keygen -f ~/.ssh/known_hosts -R ${publicHost}
	aws_report_config
	init_head_node
	connect_head
}

# Check if arg is the target arg prop, or its mapped prop name. 
# Param 1 - arg
# Param 2 - target
prop_exists() {
	[ $# -eq 2 ] && [ "$1" ==  "$2" ] || [ "$1" == $(map_property_name "$2") ]
}

# Upload local file or directory to the ec2 head node
# Param 1 - Local file
# Param 2 - AWS ec2 Head Node directory 
stage_to_head_node() {
	target=/home/ec2-user/
	[ $# -eq 2 ] && target="$2"
	scp -o StrictHostKeyChecking=no -i $(key_file) $1 ec2-user@$(get_blj_prop publicHost):$target
}

# Change the file/dir-path to the new S3 path
# Param 1 - Config file
update_config_file() {
	outFile="$AWS_CONFIG/$(basename $1)"
	touch $outFile
	#aws_log "Save new Pipeline Config with S3 paths --> $outFile"
	while read line; do
		line=$(echo ${line} | xargs)
		#aws_log "READ LINE:  ${line}"
		arg=
		val=
		if [ "${line/=/}" != "${line}" ] && [ "${line:0:1}" != "#" ]; then
			arg=$(echo ${line} | cut -d '=' -f1)
			val=$(eval "echo ${line} | cut -d '=' -f2")
			if $(prop_exists "$arg" "pipeline.defaultProps"); then
				echo "$arg=$(get_s3_name $val config)" >> $outFile	
			elif $(prop_exists "$arg" "input.dirPaths"); then
				echo "$arg=$(get_s3_name $val seq)" >> $outFile
			elif $(prop_exists "$arg" "metadata.filePath"); then
				echo "$arg=$(get_s3_name $val metadata)" >> $outFile
			elif $(prop_exists "$arg" "trimPrimers.filePath"); then
				echo "$arg=$(get_s3_name $val primers)" >> $outFile
			elif [ "$val" != "$DB" ] && $(prop_exists "$arg" "kneaddata.dbs"); then
				echo "$arg=$(get_s3_name $val db)" >> $outFile
			elif [ "$val" != "$DB" ] && $(prop_exists "$arg" "kraken.db"); then
				echo "$arg=$(get_s3_name $val db)" >> $outFile
			elif [ "$val" != "$DB" ] && $(prop_exists "$arg" "kraken2.db"); then
				echo "$arg=$(get_s3_name $val db)" >> $outFile
			elif [ "$val" != "$DB" ] && $(prop_exists "$arg" "metaphlan2.db"); then
				echo "$arg=$(get_s3_name $val db)" >> $outFile
			elif [ "$val" != "$DB" ] && $(prop_exists "$arg" "metaphlan2.mpa_pkl"); then
				echo "$arg=$(get_s3_name $val db)" >> $outFile
			elif [ "$val" != "$DB" ] && $(prop_exists "$arg" "qiime.pynastAlignDB"); then
				echo "$arg=$(get_s3_name $val db)" >> $outFile
			elif [ "$val" != "$DB" ] && $(prop_exists "$arg" "qiime.refSeqDB"); then
				echo "$arg=$(get_s3_name $val db)" >> $outFile
			elif [ "$val" != "$DB" ] && $(prop_exists "$arg" "qiime.taxaDB"); then
				echo "$arg=$(get_s3_name $val db)" >> $outFile
			elif [ "$val" != "$HN2_NUCL_DB" ] && $(prop_exists "$arg" "humann2.nuclDB"); then
				echo "$arg=$(get_s3_name $val db)" >> $outFile
			elif [ "$val" != "$HN2_PROT_DB" ] && $(prop_exists "$arg" "humann2.protDB"); then
				echo "$arg=$(get_s3_name $val db)" >> $outFile
			elif [ "$val" != "$RDP_DB" ] && $(prop_exists "$arg" "rdp.db"); then
				echo "$arg=$(get_s3_name $val db)" >> $outFile
			else
				echo "${line}" >> $outFile
				#aws_log "Pass-through line (A) --> $outFile"
			fi
		else
			echo "${line}" >> $outFile
			#aws_log "Pass-through line (B) --> $outFile"
		fi
	done < $1
	echo ${outFile}
}

# The method creates a new version of the pipeline Config files by replacing 
update_s3_config_files() {
	[ ! -d "$AWS_CONFIG" ] && mkdir $AWS_CONFIG && aws_log "Creating the staging directory:  $AWS_CONFIG"
	rm $AWS_CONFIG/*
	configFiles=$(get_blj_prop configFiles)
	aws_log "Generating new Config files with properties that reference AWS S3 locations instead of local file-paths"
	newPropFiles=
	propFiles=( ${configFiles//,/ } )
	for propFile in ${propFiles[@]}; do
		update_config_file $propFile
		s3Path=$(get_s3_name $propFile config)
		[ ${#newPropFiles} -gt 0 ] && newPropFiles=",${newPropFiles}"
		newPropFiles=${s3Path}${newPropFiles}
	done
	set_blj_prop configFiles $newPropFiles
	refresh_aws_cache
}

# Upload local directory to AWS S3 Bucket folder (if modified)
# Param 1 - Local directory
# Param 2 - S3 bucket folder
upload_s3_dir() {
	[ $# -ne 2 ] || [ "${1:0:5}" == "$S3_DIR" ] && return
	[ ! -d "$1" ] && exit_script "Error [ aws_head_node_lib.upload_s3_dir() ]: Local path is not a valid directory -->  $1"
	newPath="$(get_s3_dir $2)/$(basename $1)"
	val=$(aws s3 sync $1 $newPath --exclude *.DS_Store)
	[ ${#val} -eq 0 ] && exit_script "Error [ aws_head_node_lib.upload_s3_dir() ]: Failed to upload AWS $1 --> $newPath.\nFailed command: [ aws s3 sync $1 $newPath --exclude *.DS_Store ]"
}
 
# Upload local file to AWS S3 Bucket folder (if modified)
# Param 1 - Local file
# Param 2 - S3 bucket folder
upload_s3_file() {
	[ $# -ne 2 ] || [ "${2:0:5}" == "$S3_DIR" ] && return
	[ ! -f "$1" ] && exit_script "Error [ aws_head_node_lib.upload_s3_file() ]: Local path \"$2\"is not a valid file!"
	newPath="$(get_s3_dir $2)/$(basename $1)"
	val=$(aws s3 sync $1 $newPath --exclude *.DS_Store)
	[ ${#val} -eq 0 ] && exit_script "Error [ aws_head_node_lib.upload_s3_file() ]: Failed to upload AWS \"$1\" --> \"$newPath\".\nFailed command: \"aws s3 sync $1 $newPath --exclude *.DS_Store\""

}
