#!/bin/bash
#####################################################################
##                                                                 ##
##  This script is used for uploading data + config to AWS cloud.  ##
##                                                                 ##
##                                                                 ##
#####################################################################
. $BLJ/script/aws_upload_lib

# Add job definition to Nextflow Config
# Param 1 - Nextflow label
# Param 2 - Nextflow container
add_nextflow_job_def() {
	aws_log "Add Nextflow Docker job-definition label=${1}, container=${2}"
	echo "    withLabel: '${1}' {" >> $(get_nextflow_config)
	echo "        container = '${2}'" >> $(get_nextflow_config)
	echo "    }" >> $(get_nextflow_config)
}

# Build Docker Job defs to use with label references in Nextflow main.nf
build_docker_job_defs() {
	refresh_aws_cache
	[ -f "$(get_docker_job_flag)" ] && echo "Jobs already exist, found flag file: $(get_docker_job_flag)" && return
	init_nextflow_config
	dockerModules=$(docker search --no-trunc --limit 100 ${dockerUser} | grep -E ^${dockerUser} ) 
	IFS2=$IFS && IFS=$'\t'
	[ ${#dockerModules} -eq 0 ] && aws_log "No Docker images found for Docker Account \"${dockerUser}\"" && return
	aws_log "Building Docker job-definitions..."
	echo ${dockerModules} | while read -r line; do
		[ $(echo $line | grep -c blj_basic) -gt 0 ] && aws_log "Skip Docker image: \"$line\"" && continue
		module=$(echo $line | cut -f 1 -d " ")
		jobImg="${module}:${dockerImgVersion}"
		aws_log "Add job-definition for Docker image: \"$jobImg\""
		jobLabel=${jobImg/\//_} 
		jobLabel=${jobLabel/:/_} 
	    jobConfig="{ 
			\"image\": \"${jobImg}\",
			\"vcpus\": 2,
			\"memory\": 1024,
			\"command\": [ \"/bin/bash\" ],
			\"jobRoleArn\": \"$(get_stack_param ECSTaskRole)\",
			\"volumes\": [ { \"host\": { \"sourcePath\": \"${EFS}\" }, \"name\": \"efs\" }, { \"host\": { \"sourcePath\": \"${EC2_HOME}/miniconda\" }, \"name\": \"miniconda\" } ],
			\"mountPoints\": [ { \"containerPath\": \"${EFS}\", \"readOnly\": false, \"sourceVolume\": \"efs\" }, { \"containerPath\": \"${EC2_HOME}/miniconda\", \"readOnly\": true, \"sourceVolume\": \"miniconda\" } ],
			\"readonlyRootFilesystem\": false,
			\"privileged\": true
		}"
		jobDef="${awsStack}_${jobLabel}"
		jobName=$(aws batch describe-job-definitions --status ACTIVE --job-definition-name ${jobDef} --query "jobDefinitions[*].jobDefinitionName")
		if [ ${#jobName} -eq 0 ]; then
			registeredJob=$(aws batch register-job-definition --job-definition-name $jobDef --type container --container-properties "${jobConfig}")
			aws_log "Registered new Docker job-definition: ${registeredJob}"
			jobName=$(echo $registeredJob | grep job-definition | sed 's/^.*job-definition/job-definition:\//' | awk '//{print $1}' )
			[ ${#jobName} -eq 0 ] && exit_script "Error [ aws_head_node_lib.build_docker_job_defs() ]: Failed to register job-definition: ${jobDef}"
			aws_log "Docker job-definition name: ${jobName}"
		else
			aws_log "Found existing Docker job-definition: ${jobName}"
		fi
		add_nextflow_job_def "image_${jobLabel}" "${jobName}"
	done
	IFS2=$IFS
	close_nextflow_config
	touch $(get_docker_job_flag)
}

# Build start script for given Pipeline Config file and reutnr local path
# Param 1 - Pipeline Config file
build_start_script() {
	startScript=~/.aws/run-${awsStack}.sh
	[ -f "${metadataFilePath}" ] && meta="-m=$EFS_META "
	echo $sheBang > $startScript
	echo "" >> $startScript
	echo "##########################################################" >> $startScript
	echo "##                                                      ##" >> $startScript
	echo "##  Run this script to launch BioLockJ pipeline on AWS  ##" >> $startScript
	echo "##  1) Clear old biolockj Docker images (if any)        ##" >> $startScript
	echo "##  2) Pull the latest biolockj image from Docker Hub   ##" >> $startScript
	echo "##  3) Run dockblj                                      ##" >> $startScript
	echo "##                                                      ##" >> $startScript
	echo "##########################################################" >> $startScript
	echo 'source $HOME/.bash_profile' >> $startScript
	echo "clearDock" >> $startScript
	echo 'rm $(find $EFS -name .DS_Store)' >> $startScript
	echo "set_prop $(get_config ec2) \"cluster.host\" aws" >> $startScript
	echo "docker pull ${dockerUser}/biolockj_controller:${dockerImgVersion}" >> $startScript
	echo "dockblj ${meta}-i=$EFS_INPUT/$(basename ${inputDirPaths}) -c=$(get_config ec2)" >> $startScript
	[ "${awsCopyInputsToS3}" == "Y" ] && echo "aws_sync_inputs_to_s3" >> $startScript
	[ "${awsCopyPipelineToS3}" == "Y" ] && echo "aws_sync_outputs_to_s3" >> $startScript
	[ "${awsPurgeEfsInputs}" == "Y" ] && echo 'sudo rm -rf $EFS/[cdims]*/*; sudo rm -rf $EFS/primer' >> $startScript
	[ "${awsPurgeEfsOutput}" == "Y" ] && echo 'sudo rm -rf $BLJ_PROJ/*' >> $startScript
	chmod 770 $startScript
	echo $startScript
}

# Close Nextflow Config process block
close_nextflow_config() {
	echo "}" >> $(get_nextflow_config)
	echo "" >> $(get_nextflow_config)
	#chmod 777 $(get_nextflow_config)
}

# Connect to running head node
connect_head() {
	publicIP=$(get_ec2_public_ip)
	aws_log "Opening SSH tunnel to Head Node...$publicIP"
	ssh -o StrictHostKeyChecking=no -i $(key_file) ec2-user@$publicIP
}

# Get the Pipeline Config local file-path
# Param 1 - local or ec2
get_config() {
	configFiles=$(get_blj_prop configFiles)
	propFiles=( ${configFiles//,/ } )
	numConfig=${#propFiles[@]}
	((numConfig--))
	[ "${1}" == "local" ] && echo "${propFiles[$numConfig]}"
	[ "${1}" == "ec2" ] && echo "$EFS_CONFIG/$(basename ${propFiles[$numConfig]})"
}

# Get the flag file that indicates Docker job definitions were created
get_docker_job_flag() {
	echo ~/.aws/.$(get_blj_prop awsStack)-CREATED_DOCKER_JOB_DEFS
}

# Get a stack parameter for the configured stack 
# Param 1 - Param name
get_stack_param() {
	myParam=$(aws cloudformation describe-stacks --stack-name "${awsStack}" --query "Stacks[*].Outputs[?OutputKey=='$1'].OutputValue")
	[ ${#myParam} -eq 0 ] && return
	echo "${myParam}"
}

# Create the basic nextflow config file using the current statck AWS batch queues
init_nextflow_config() {
	echo "// Nextflow properties inherited by main.nf in BioLockJ $(get_version) pipelines" > $(get_nextflow_config)
	echo "" >> $(get_nextflow_config)
	echo "executor {" >> $(get_nextflow_config)
    echo "    name = 'awsbatch'" >> $(get_nextflow_config)
    echo "    executor.awscli = '/home/ec2-user/miniconda/bin/aws'" >> $(get_nextflow_config)
    echo "}" >> $(get_nextflow_config)
    echo "" >> $(get_nextflow_config)
    echo "aws {" >> $(get_nextflow_config)
	echo "    region = '${awsRegion}'" >> $(get_nextflow_config)
	echo "}" >> $(get_nextflow_config)
	echo "" >> $(get_nextflow_config)
	echo "process {" >> $(get_nextflow_config)
	echo "    queue = '$(get_stack_param LowPriorityJobQueue)'" >> $(get_nextflow_config)
	echo "    withLabel: 'DEMAND' {" >> $(get_nextflow_config)
	echo "        queue = '$(get_stack_param HighPriorityJobQueue)'" >> $(get_nextflow_config)
	echo "    }" >> $(get_nextflow_config)
}

set_ec2_head_node_id() {
	refresh_aws_cache
	isStarting=false
	[ "${#awsEc2InstanceID}" -gt 0 ] && instances=$(aws ec2 describe-instances --instance-ids "${awsEc2InstanceID}" --query "Reservations[].Instances[].[State.Name, InstanceId]")
	if [ "${instances/$awsEc2InstanceID}" != "${awsEc2InstanceID}" ]; then
		echo "Found existing instance ${awsEc2InstanceID}"
		if [ "${instances/stopped}" != "${instances}" ]; then
			aws ec2 start-instances --instance-ids "${awsEc2InstanceID}"
			printf "Configured EC2 instance ${awsEc2InstanceID} was stopped...attempting to restart...please wait."
			isStarting=true
		elif [ "${instances/terminated}" != "${instances}" ]; then
			echo "Configured EC2 instance ${awsEc2InstanceID} was terminated...launching a new instance"
		elif [ "${instances/pending}" != "${instances}" ] || [ "${instances/shutting-down}" != "${awsEc2InstanceID}" ] || [ "${instances/stopping}" != "${awsEc2InstanceID}" ]; then
			exit_script "Configured EC2 Instance ${awsEc2InstanceID} has an in-between state-status, aborting launch EC2 instance - check status and try again later: ${instances}"
		elif [ "${instances/running}" != "${instances}" ]; then
			echo "Configured EC2 instance ${awsEc2InstanceID} is running..." && return
		fi
	fi
	
	runTime=0
	if [ $isStarting == false ]; then 
		secureGroup="$(get_stack_param BastionSecurityGroup)"
		subnet="$(get_stack_param Subnet1)"
		templateId="$(get_stack_param HeadNodeLaunchTemplateId)"
		if [ ${#awsAmi} -eq 0 ] || [ ${#awsStack} -eq 0 ] || [ ${#secureGroup} -eq 0 ] || [ ${#subnet} -eq 0 ] || [ ${#templateId} -eq 0 ]; then
			exit_script "Error [ aws_head_node_lib.launch_ec2_head_node() ]: Cannot launch EC2 - required parameters undefined in $blj_aws_config"
		fi
		awsEc2InstanceID=$(aws ec2 run-instances --count 1 --key-name ${awsStack} --image-id ${awsAmi} --security-group-ids ${secureGroup} \
			--tag-specifications "ResourceType=instance,Tags={Key=Name,Value='Head-${awsStack}'}" --subnet-id ${subnet} \
			--launch-template LaunchTemplateId=${templateId} --instance-type "${awsEc2InstanceType}" --query "Instances[].InstanceId" )
		printf "Launching EC2 Instance, please wait."
	fi
	systemStatus=init  
	while [ "$systemStatus" != "ok" ]; do
		systemStatus=$(aws ec2 describe-instance-status --instance-ids ${awsEc2InstanceID} --query "InstanceStatuses[*].SystemStatus.Status")
		printf "."
		sleep 10s
		runTime=$((runTime+10))  
	done
	echo ""
	aws_log "EC2 Instance [ ID=${awsEc2InstanceID} ] created in $runTime seconds"
	set_blj_prop awsEc2InstanceID ${awsEc2InstanceID}
}

# Get EC2 instance public IP address
get_ec2_public_ip() {
	[ "${#ec2PublicIp}" -eq 0 ] && ec2PublicIp=$(aws ec2 describe-instances --instance-ids ${awsEc2InstanceID} --query "Reservations[].Instances[].PublicDnsName")
	set_blj_prop ec2PublicIp "${ec2PublicIp}"
	echo "${ec2PublicIp}"
}

# Launch a new ec2 head node
# Param 1 (optional) - If "start" then launch the pipeline, otherwise connect to the head node
launch_ec2_head_node() {
	[ -x "$(which docker)" ] && build_docker_job_defs
	set_ec2_head_node_id
	refresh_aws_cache
	keyFound=$(cat ~/.ssh/known_hosts | grep -c $(get_ec2_public_ip))
	[ ${keyFound} -gt 0 ] && ssh-keygen -f ~/.ssh/known_hosts -R $(get_ec2_public_ip)
	startScript=$(build_start_script)
	stage_pipeline_on_aws $startScript
	if [ -x "$(which docker)" ] && [ "${1}" == "start" ]; then
		exe_remote_cmd "${EC2_HOME}/$(basename $startScript)"
	else
		connect_head
	fi
}
