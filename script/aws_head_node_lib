#!/bin/bash
#####################################################################
##                                                                 ##
##  This script is used for uploading data + config to AWS cloud.  ##
##                                                                 ##
##  s3://$awsS3           - Top level $BLJ bucket                  ##
##  s3://$awsS3/config    - $BLJ_CONFIG folder with DB sub-folders ##
##  s3://$awsS3/db        - $BLJ_DB folder with DB sub-folders     ##
##  s3://$awsS3/metadata  - $BLJ_META folder for metadata files    ##
##  s3://$awsS3/pipelines - $BLJ_PROJ folder for pipeline output   ##
##  s3://$awsS3/primers   - $BLJ_PRIMER folder for seq primers     ##
##  s3://$awsS3/seq       - $BLJ_SEQ folder for sequence files     ##
##                                                                 ##
#####################################################################
. $BLJ/script/aws_config_lib

AWS_CONFIG=~/.aws_config
S3_DIR="s3://"

# Sync local files to S3 for cloud pipeline execution
aws_sync_s3_folders() {
	[ ${#awsS3} -eq 0 ] && exit_script "Error [ aws_head_node_lib.aws_sync_s3_folders() ]: Requires awsS3 property is defined in $blj_aws_config"
	aws_upload_s3 inputDirPath $inputDirPath seq
	aws_upload_s3 metadataFilePath $metadataFilePath metadata
	aws_upload_s3 trimPrimersFilePath $trimPrimersFilePath primers
	aws_upload_s3 humann2NuclDB $humann2NuclDB db
	aws_upload_s3 humann2ProtDB $humann2ProtDB db
	aws_upload_s3 kneaddataDbs $kneaddataDbs db
	aws_upload_s3 krakenDb $krakenDb db
	aws_upload_s3 kraken2Db $kraken2Db db
	aws_upload_s3 metaphlan2Db $metaphlan2Db db
	aws_upload_s3 metaphlan2Mpa_pkl $metaphlan2Mpa_pkl db
	aws_upload_s3 qiimePynastAlignDB $qiimePynastAlignDB db
	aws_upload_s3 qiimeRefSeqDB $qiimeRefSeqDB db
	aws_upload_s3 qiimeTaxaDB $qiimeTaxaDB db
	aws_upload_s3 rdpDb $rdpDb db
	
	configFiles=$(get_blj_prop configFiles)
	newConfigFiles=
	IFS=","
	propFiles=( $configFiles )
	for propFile in ${propFiles[@]}; do
		val=$(aws_upload_s3 configFiles $propFile config)
		[ ${#val} -gt 0 ] && [ ${#newConfigFiles} -gt 0 ] && newConfigFiles=",$newConfigFiles"
		[ ${#val} -gt 0 ] && newConfigFiles="${val},${newConfigFiles}"
	done
	[ ${#newConfigFiles} -gt 0 ] && set_blj_prop configFiles $newConfigFiles
}

# List all contents of an AWS S3 Bucket
# TEST: aws_s3_ls $awsS3/r16s_fastq
# Param 1 - S3 bucket folder name
aws_s3_ls() {
	aws s3 ls s3://$1 --recursive --human-readable
}

# Sync local directory (all files/dirs) with AWS S3 Bucket-path
# TEST: aws_upload_s3 inputDirPath $BLJ_SUP/resources/test/data/r16s_fastq seq
# Param 1 - AWS property name
# Param 2 - Local Directory of File
# Param 3 - S3 bucket folder
aws_upload_s3() {
	[ $# -eq 2 ] && [ "$1" != "$IS_NULL" ] && return
	[ $# -lt 2 ] || [ "${2:0:5}" == "$S3_DIR" ] && return
	[ ! -f "$2" ] && [ ! -d "$2" ] && exit_script "Error [ aws_head_node_lib.aws_upload_s3() ]: Local path \"$2\" not found!"
	newPath="$(get_s3_name $2 $3)"
	[ ${#newPath} -eq 0 ] && exit_script "Error [ aws_head_node_lib.aws_upload_s3() ]: Unable to build S3 AWS property for $1=\"$2\".\nPlease verify \"$2\" is a valid file-path"
	
	val=$(aws s3 sync $2 $newPath --exclude *.DS_Store)
	if [ ${#val} -gt 0 ]; then
		set_blj_prop $1 $newPath
	else
		exit_script "Error [ aws_head_node_lib.aws_upload_s3() ]: Failed to update AWS property $1=\"$newPath\" in --> $blj_aws_config.\nFailed command: \"aws s3 sync $2 $newPath --exclude *.DS_Store\""
	fi
	
	echo ${newPath}
}

# Build Docker Job defs to use with label references in Nextflow main.nf
build_docker_job_defs() {
	dockerModules=$(docker search $dockerUser | grep -E ^$dockerUser ) 
	jobVcpus=2
	jobRam=1024
	IFS=$'\t'
	[ ${#dockerModules} -eq 0 ] && "No Docker images found for Docker Account \"$dockerUser\"" && return
	aws_log "Building job-definitions"
	echo $dockerModules | while read -r line; do
		[ $(echo $line | grep -c blj_basic) -gt 0 ] && continue
		module=$(echo $line | cut -f 1 -d " ")
		jobImg="${module}:${dockerImgVersion}"
		jobLabel=${jobImg/\//_} 
		jobLabel=${jobLabel/:/_} 
	    jobConfig="{ 
	    		\"image\": \"$jobImg\",
			\"vcpus\": $jobVcpus,
			\"memory\": $jobRam,
			\"command\": [ \"/bin/bash\" ],
			\"jobRoleArn\": \"$(get_stack_param ECSTaskRole)\",
			\"volumes\": [ { \"host\": { \"sourcePath\": \"/mnt/efs\" }, \"name\": \"efs\" } ],
			\"mountPoints\": [ { \"containerPath\": \"/efs\", \"readOnly\": false, \"sourceVolume\": \"efs\" } ],
			\"readonlyRootFilesystem\": false,
			\"privileged\": true
		}"
		jobDef="${awsStack}_${jobLabel}"
		jobName=$(aws batch describe-job-definitions --status ACTIVE --job-definition-name $jobDef --query "jobDefinitions[*].jobDefinitionName")
		if [ ${#jobName} -eq 0 ]; then
			registeredJob=$(aws batch register-job-definition --job-definition-name $jobDef --type container --container-properties "${jobConfig}")
			jobName=$(echo $registeredJob | grep job-definition | sed 's/^.*job-definition/job-definition:\//' | awk '//{print $1}' )
			aws_log "Registered new Docker job-definition: $jobName"
		else
			aws_log "Found existing Docker job-definition: $jobName"
		fi
		prop=$(get_blj_prop "image_$jobLabel" $jobName)
		aws_log "Saved Docker job-definition label: image_$jobLabel=$prop"
	done
	touch ~/.${awsStack}-CREATED_DOCKER_JOB_DEFS
}

# Connect to running head node
connect_head() {
	aws_log "Opening SSH tunnel to Head Node...$(get_blj_prop publicHost)"
	ssh -o StrictHostKeyChecking=no -i $(key_file) ec2-user@$(get_blj_prop publicHost)
	
	#mv ~/.bash_profile ~/.bash_profile~
	#mv ~/ec2_head_node_profile ~/.bash_profile
	#. ~/.bash_profile
	#build_docker_job_defs
}

# Get the Pipeline Config file staged for AWS
# Param 1 - Local Config file
get_aws_config_file() {
	echo ~/.aws_config/$(basename $1)
}

# Param 1 - Local Directory of File
# Param 2 - S3 bucket folder
get_s3_name() {
	[ ! -f "$1" ] && [ ! -d "$1" ] && return
	echo "s3://$awsS3/$2/$(basename $1)"
}

# Execute remote commands
# Param 1 - Remote command
exe_remote() {
	ssh -o StrictHostKeyChecking=no -i $(key_file) ec2-user@$(get_blj_prop publicHost) "$1"
}

# Upload local files to S3
# Copy Config file with updated properties to reference new S3 paths
init_head_node() {
	refresh_aws_cache
	update_config_s3_props
	aws_sync_s3_folders
	
	stage_to_head_node $BLJ/resources/aws/ec2_head_node_profile 
	stage_to_head_node $blj_aws_config
	stage_to_head_node $BLJ/script
	
	exe_remote "mv ~/.bash_profile ~/.bash_profile~; mv ~/ec2_head_node_profile ~/.bash_profile"
	
	#exe_remote "[ ! -d /home/ec2-user/config ] && mkdir /home/ec2-user/config"
	#configFiles=$(get_blj_prop configFiles)
	#IFS=","
	#propFiles=( $configFiles )
	#for propFile in ${propFiles[@]}; do
	#	stage_to_head_node $propFile /home/ec2-user/config
	#done
}

# Launch a new ec2 head node
launch_ec2_head_node() {
	refresh_aws_cache
	[ -x "$(which docker)" ] && build_docker_job_defs

	runTime=0
	instanceID=$(aws ec2 run-instances --count 1 --key-name ${awsStack} --image-id ${awsAmi} --security-group-ids $(get_stack_param BastionSecurityGroup)  \
		--tag-specifications "ResourceType=instance,Tags={Key=Name,Value='HeadNode'}" --subnet-id $(get_stack_param Subnet1) \
		--launch-template LaunchTemplateId=$(get_stack_param HeadNodeLaunchTemplateId) --instance-type $awsEc2InstanceType --query "Instances[].InstanceId" )

	printf "Launching EC2 Instance, please wait."
	systemStatus=init
	while [ "$systemStatus" != "ok" ]; do
		systemStatus=$(aws ec2 describe-instance-status --instance-ids $instanceID --query "InstanceStatuses[*].SystemStatus.Status")
		printf "."
		sleep 10s
		runTime=$((runTime+10))  
	done
	aws_log "InstanceID [ $instanceID ] created in $runTime seconds!"
	set_blj_prop instanceID $instanceID
	set_blj_prop publicHost $(aws ec2 describe-instances --instance-ids $instanceID --query "Reservations[].Instances[].PublicDnsName")
	
	# THESE APPEAR TO NEVER BE USED, COMMENTING OUT FOR NOW Apr 4
	#set_blj_prop privateHost $(aws ec2 describe-instances --instance-ids $instanceID --query "Reservations[].Instances[].PrivateDnsName")
	#set_blj_prop privateIp $(aws ec2 describe-instances --instance-ids $instanceID --query "Reservations[].Instances[].PrivateIpAddress")
	#set_blj_prop publicIp $(aws ec2 describe-instances --instance-ids $instanceID --query "Reservations[].Instances[].PublicIpAddress")
	
	publicHost=$(get_blj_prop publicHost)
	keyFound=$(cat ~/.ssh/known_hosts| grep -c ${publicHost})
	[ $keyFound -gt 0 ] && ssh-keygen -f ~/.ssh/known_hosts -R ${publicHost}
	aws_report_config
	init_head_node
	connect_head
}

# Remove AWS property - delete the line from the file
# Param 1 - Config file
# Param 2 - prop name 
rm_config_prop() {
	conf=$(cat $1)
	[ ${#conf} -eq 0 ] && return
	TMP=~/.temp_config.txt
	[ -f $TMP ] && rm $TMP
	touch $TMP && chmod 770 $TMP
	cat $1 | while read -r line; do
		if [ ${#line} -eq 0 ] || [ "${line:0:1}" == "#" ]; then
			echo ${line} >> $TMP
		else
			IFS2=$IFS && IFS="=" && tokens=( ${line} )
			IFS=$IFS2 && [ "${tokens[0]}" != "$2" ] && echo ${line} >> $TMP
		fi
	done
	rm $1 && [ -f $TMP ] && mv $TMP $1
}

# Set a prop stored in $blj_aws_config.  In case prop value ${2} is null, $2 actually returns the 3rd param (the default)
# Param 1 Prop name
# Param 2 Prop value
# Param 3 Config file
set_config_prop() {
	[ $# -eq 2 ] && return
	prop=$(get_prop $3 $1) 
	exists=$(echo $prop | grep "$1=")
	[ ${#exists} -gt 0 ] && return
	rm_config_prop $3 $1
	echo "$1=$2" >> $3
}

# Upload local file or directory to the ec2 head node
# Param 1 - Local file
# Param 2 - AWS ec2 Head Node directory 
stage_to_head_node() {
	target=/home/ec2-user/
	[ $# -eq 2 ] && target="$2"
	scp -o StrictHostKeyChecking=no -i $(key_file) $1 ec2-user@$(get_blj_prop publicHost):$target
}

# Change the file/dir-path to the new S3 path
# Param 1 - Config file
update_config_file() {
	outFile=$(get_aws_config_file $1)
	cat $1 | while read -r line; do
		IFS2=$IFS && IFS="=" && tokens=( ${line} ) && IFS=$IFS2
		[ ${#tokens[@]} -gt 0 ] && arg=${tokens[0]}
		[ ${#tokens[@]} -gt 1 ] && val=${tokens[1]}
		if [ ${#tokens[@]} -eq 1 ] || [ ${#tokens[@]} -eq 2 ] && [ "${2:0:5}" == "$S3_DIR" ]; then
			echo "$line" >> $outFile
		elif [ "$arg" == "pipeline.defaultProps" ] && [ -d "$val" ]; then
			newPath=$(get_s3_name $defaultConfig config)
			echo "$arg=$newPath" >> $outFile
		elif [ "$arg" == "input.dirPaths" ] && [ -d "$val" ]; then
			newPath=$(aws_upload_s3 inputDirPath $inputDirPath seq)
			echo "$arg=$newPath" >> $outFile
		elif [ "$arg" == "metadata.filePath" ] && [ -f "$val" ]; then
			newPath=$(aws_upload_s3 metadataFilePath $metadataFilePath metadata)
			echo "$arg=$newPath" >> $outFile
		elif [ "$arg" == "trimPrimers.filePath" ] && [ -f "$val" ]; then
			newPath=$(aws_upload_s3 trimPrimersFilePath $trimPrimersFilePath primers)
			echo "$arg=$newPath" >> $outFile
		elif [ "$arg" == "humann2.nuclDB" ] && [ -d "$val" ]; then
			newPath=$(aws_upload_s3 humann2NuclDB $humann2NuclDB db)
			echo "$arg=$newPath" >> $outFile
		elif [ "$arg" == "humann2.protDB" ] && [ -d "$val" ]; then
			newPath=$(aws_upload_s3 humann2ProtDB $humann2ProtDB db)
			echo "$arg=$newPath" >> $outFile
		elif [ "$arg" == "kneaddata.dbs" ] && [ -d "$val" ]; then
			newPath=$(aws_upload_s3 kneaddataDbs $kneaddataDbs db)
			echo "$arg=$newPath" >> $outFile
		elif [ "$arg" == "kraken.db" ] && [ -d "$val" ]; then
			newPath=$(aws_upload_s3 krakenDb $krakenDb db)
			echo "$arg=$newPath" >> $outFile
		elif [ "$arg" == "kraken2.db" ] && [ -d "$val" ]; then
			newPath=$(aws_upload_s3 kraken2Db $kraken2Db db)
			echo "$arg=$newPath" >> $outFile
		elif [ "$arg" == "metaphlan2.db" ] && [ -d "$val" ]; then
			newPath=$(aws_upload_s3 metaphlan2Db $metaphlan2Db db)
			echo "$arg=$newPath" >> $outFile
		elif [ "$arg" == "metaphlan2.mpa_pkl" ] && [ -f "$val" ]; then
			newPath=$(aws_upload_s3 metaphlan2Mpa_pkl $metaphlan2Mpa_pkl db)
			echo "$arg=$newPath" >> $outFile
		elif [ "$arg" == "qiime.pynastAlignDB" ] && [ -f "$val" ]; then
			newPath=$(aws_upload_s3 qiimePynastAlignDB $qiimePynastAlignDB db)
			echo "$arg=$newPath" >> $outFile
		elif [ "$arg" == "qiime.refSeqDB" ] && [ -f "$val" ]; then	
			newPath=$(aws_upload_s3 qiimeRefSeqDB $qiimeRefSeqDB db)
			echo "$arg=$newPath" >> $outFile
		elif [ "$arg" == "qiime.taxaDB" ] && [ -f "$val" ]; then
			newPath=$(aws_upload_s3 qiimeTaxaDB $qiimeTaxaDB db)
			echo "$arg=$newPath" >> $outFile
		elif [ "$arg" == "rdp.db" ] && [ -f "$val" ]; then
			newPath=$(aws_upload_s3 rdpDb $rdpDb db)
			echo "$arg=$newPath" >> $outFile
		else
			echo "$line" >> $outFile
		fi
	done
	echo ${outFile}
}

# The method creates a new version of the pipeline Config files by replacing 
update_config_s3_props() {
	[ ! -d "$AWS_CONFIG" ] && mkdir $AWS_CONFIG
	configFiles=$(get_blj_prop configFiles)
	IFS=","
	propFiles=( $configFiles )
	newPropFiles=
	for propFile in ${propFiles[@]}; do
		newLocalConfig=$(update_config_file $propFile)
		newPath=$(aws_upload_s3 $IS_NULL $newLocalConfig config)
		[ ${#newPropFiles} -gt 0 ] && newPropFiles=",$newPropFiles"
		newPropFiles=${newPath}${newPropFiles}
	done
	set_config_prop configFiles $newPropFiles
}
